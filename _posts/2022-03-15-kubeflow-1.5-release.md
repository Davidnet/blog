---
title: "Kubeflow v1.5 improves ML model accuracy, reduces infrastructure costs and optimizes MLOps"
layout: post
toc: true
comments: true
image: images/logo.png
hide: false
categories: [release]
permalink: /kubeflow-1.5-release/
author: "Josh Bottum, Kimonas Sotirchos, Thea Lamkin"
---

The Kubeflow v1.5 software release improves ML model accuracy, lowers infrastructure costs, and simplifies operations by providing a more consistent user experience between components.

## Lower infrastructure costs and improve model accuracy

There are several enhancements in Kubeflow v1.5 that lower infrastructure costs and help improve model accuracy.  For example, the new elastic training functionality augments the Kubeflow training operator for PyTorch and enables PyTorch workers to be scaled up and down, providing fault tolerant and elastic training.  This allows the training jobs to continue without restarting from scratch even if a worker fails.  The elastic training functionality also can enable the use of ephemeral or spot instances, which can save infrastructure costs. Please find more details on elastic training [here] (https://github.com/kubeflow/community/blob/b56452405e44c05ed60145a19bb86be55f3833d5/proposals/pytorch-elastic-proposal.md)   

In addition, v1.5 extends the notebook culling feature, which monitors use on Kubeflow notebook server’s kernel, and shuts down the notebook servers that have been inactive when a configurable timer expires.  The kernel monitoring provides more precision than the previous implementation, which monitored the notebook server’s UI activity.   Additionally, v1.5 includes [early validation of hyperparameter tuning](https://github.com/kubeflow/katib/pull/1709).  This feature improves model accuracy by reducing the overfitting of the model to the data sets used during hyperparameter tuning.  It can also reduce the infrastructure used as it will stop hyperparameter tuning when configurable thresholds are reached.

## Simplified operations

v1.5 simplifies operations in several ways including easier support of high availability of Katib Controller via the new hyperparameter leader [election](https://github.com/kubeflow/katib/pull/1713).  v1.5 also adds the MPI framework to the Unified Training Operator, which  enables users to leverage a single distributed training CRD for the most popular frameworks: Tensorflow, Pytorch, MXNet, XGBoost and MPI.  Kubeflow Pipelines (KFP) has incorporated support for the [Emissary Executor] (https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#emissary-executor) as the default executor, which means that users are not required to manually configure this option to Emissary.  This enhancement also enables KFP to support the newer Kubernetes versions that require a Container Runtime Interface (CRI) and provides support for Docker containers.  

In addition, KServe v0.7 is integrated with Kubeflow v1.5.  For users who are still using KFServing v0.6.1, v1.5 does support the installation of the KFServing v0.6.1 release.  For users migrating from KFServing to KServe, there is helpful documentation,which you can find [here](https://kserve.github.io/website/0.7/admin/migration/).  From a feature standpoint, KServe v0.7 has added a beta of [Model Mesh](https://github.com/kserve/modelmesh-serving), which enables easier scaling of model serving and provides an architecture that overcomes pod and IP limitations that limit the number of models that can run on a single node and/or cluster.

Note on K8s 1.22: Although many of the Kubeflow services (KFP, AutoML, Training Operator, KServe) have started testing with K8 1.22, v1.5 was not thoroughly tested with K8s 1.22.   Additionally, critical work is still outstanding for Kubeflow Notebooks and other central services.  Although the Community always encourages testing, K8s 1.22 is not formally supported with Kubeflow v1.5.  K8s 1.22 support is on the roadmap for Kubeflow’s next release and if you are interested in K8s 1.22 support, please add a comment on this tracking [issue](https://github.com/kubeflow/kubeflow/issues/6098).

## Streamlined user experience

In v1.5, the Kubeflow User Interface (UI) dashboards for Notebooks, AutoML and Volume Manager more closely match KFP’s dashboard.   These enhancements include how dashboard fields are displayed and how operations are performed.  This gives users a consistent look and feel when working in the UI.   Additionally, Kubeflow notebooks and the volume manager have implemented a new configuration template for configuring and using persistent volumes and Kubernetes storage classes.  This template makes it easier to expose more storage classes and will enable easier support of popular storage offerings, including NFS.  In addition, AutoML has also updated its SDK, CI framework and parameter settings across frameworks (goptuna, optuna, hyperopt).

## Installation, tutorials and documentation

The Kubeflow v1.5 release process includes improvements to Kubeflow’s installation, tutorial and documentation.  For installation, the Manifest Working Group’s [documentation] (https://github.com/kubeflow/manifests) provided the starting place for the Kubeflow [Distributions](https://www.kubeflow.org/docs/started/installing-kubeflow/), and this collaboration has increased the maturity of the code and documentation for Kubeflow’s GitOps installation pattern.  The v1.5 release process included a tracking [issue]  (https://github.com/kubeflow/website/issues/3130) for the critical web pages that are updated in each release.  Additionally, there are many Working Group documentation updates and we invite you to open issues when you find documentation that needs improvement.  Kubeflow v1.5 also includes two tutorials, which are described below:

**Tutorial 1 - E2E MNIST with Kubeflow**
[MNIST Tutorial](https://github.com/kubeflow/pipelines/blob/master/samples/contrib/kubeflow-e2e-mnist/kubeflow-e2e-mnist.ipynb),which provides an end-to-end test sequence i.e. start a notebook, run a pipeline, execute training, hyperparameter tuning and model serving. 

**Tutorial 2 - Deploy and run your first interfere service with KServe**
[KServe Tutorial](https://github.com/js-ts/website-1/blob/master/content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb) Run your first interface service on Kubeflow 1.5 i.e. define, create and check an InferenceService, post an interfere request and receive response.  Optionally run performance tests.
 
## Join the community

The v1.5 Release Team would like to thank everyone for their efforts on Kubeflow v1.5, especially the users, code contributors, working group leads, and distribution team leads. As you can see from the extensive contributions to Kubeflow 1.5, the Kubeflow Community is vibrant and diverse, and solving real world problems for organizations around the globe.

Excited to contribute your great ideas? The Kubeflow Community [Working Groups](https://github.com/kubeflow/community/blob/master/wg-list.md) hold open meetings, public lists, and are always looking for more volunteers and users to unlock the potential of machine learning. If you’re interested in becoming a Kubeflow contributor, please feel free to check out the resources below. We look forward to working with you!

- Visit our [Kubeflow website](https://www.kubeflow.org/) or [Kubeflow Github page](https://github.com/kubeflow)
- Join the [Kubeflow Slack Channel](https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg)
- Join the [kubeflow-discuss](https://groups.google.com/forum/#!forum/kubeflow-discuss) mailing list
- Attend a [weekly community meeting](https://www.kubeflow.org/docs/about/community/) 
